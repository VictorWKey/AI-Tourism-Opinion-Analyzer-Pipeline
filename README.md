# AI Tourism Opinion Analyzer - Production Pipeline

Sistema automatizado de anÃ¡lisis de opiniones turÃ­sticas con soporte para LLM local o API.

## ğŸš€ Inicio RÃ¡pido

### â­ OpciÃ³n 1: Setup AutomÃ¡tico Completo (TODO EN UNO) - RECOMENDADO

Este script hace **TODO** por ti automÃ¡ticamente:
- âœ… Instala todas las dependencias Python
- âœ… Instala Ollama (LLM local)
- âœ… Descarga el modelo LLM que elijas
- âœ… Configura el archivo `.env`
- âœ… Descarga datos necesarios (NLTK)
- âœ… Prueba que todo funcione

```bash
cd production
# Paso 1: Instalar dependencias Python
./scripts/install_dependencies.sh

# Paso 2: Configurar LLM local
./scripts/setup_local_llm_completo.sh

# Paso 3: Â¡Listo! Ejecutar pipeline
python main.py
```

**Â¡Eso es todo!** En unos minutos tendrÃ¡s todo funcionando 100% gratis.

### ğŸ”§ OpciÃ³n 2: InstalaciÃ³n Manual por Pasos

Si prefieres hacerlo paso a paso:

```bash
# 1. Ejecutar script de instalaciÃ³n Ollama
./scripts/setup_ollama.sh

# 2. Descargar modelo
ollama pull llama3.2:3b

# 3. Configurar .env para modo local
cp .env.example .env
# Editar .env: USE_API=false, OLLAMA_MODEL=llama3.2:3b

# 4. Instalar dependencias Python
pip install -r requirements.txt

# 5. Ejecutar pipeline
python main.py
```

### ğŸ¯ Control de EjecuciÃ³n de Fases

El pipeline ahora permite controlar quÃ© fases se ejecutan mediante el diccionario `CONFIG_FASES` en `main.py`:

```python
CONFIG_FASES = {
    'fase_01': True,   # Procesamiento BÃ¡sico
    'fase_02': True,   # AnÃ¡lisis de Sentimientos
    'fase_03': True,   # AnÃ¡lisis de Subjetividad
    'fase_04': True,   # ClasificaciÃ³n de CategorÃ­as
    'fase_05': True,   # AnÃ¡lisis JerÃ¡rquico de TÃ³picos
    'fase_06': True,   # Resumen Inteligente
    'fase_07': True,   # GeneraciÃ³n de Visualizaciones
}
```

**Comportamiento:**
- `True` = La fase se ejecuta siempre (incluso si ya fue ejecutada)
- `False` = La fase se omite SI ya fue ejecutada previamente
- **Inteligente:** Si una fase NO ha sido ejecutada nunca, se ejecutarÃ¡ automÃ¡ticamente sin importar la configuraciÃ³n

Esto permite:
- âœ… Re-ejecutar solo fases especÃ­ficas sin procesar todo
- âœ… Ahorrar tiempo omitiendo fases ya completadas
- âœ… Desarrollo iterativo mÃ¡s eficiente

### ğŸ’³ OpciÃ³n 3: Modo API (OpenAI - Pago)

```bash
# 1. Copiar archivo de configuraciÃ³n
cp .env.example .env

# 2. Editar .env y configurar:
#    - LLM_MODE=api
#    - OPENAI_API_KEY=tu-api-key

# 3. Instalar dependencias Python
pip install -r requirements.txt

# 4. Ejecutar pipeline
python main.py
```

## ğŸ“‹ Requisitos

### Requisitos Comunes
- Python 3.10+
- 4GB RAM mÃ­nimo (8GB recomendado)
- GPU NVIDIA (opcional, mejora velocidad)

### Requisitos Adicionales por Modo

#### Modo Local (Ollama)
- **Espacio en disco**: 2-5 GB para modelos
- **RAM adicional**: 1-5 GB segÃºn modelo
- Sin costo por uso âœ…

#### Modo API (OpenAI)
- **Internet**: ConexiÃ³n estable
- **API Key**: Cuenta de OpenAI
- Costo por uso (~$0.15 por 1M tokens) ğŸ’°

## ğŸ“š DocumentaciÃ³n Completa

Para configuraciÃ³n detallada de LLM, consulta: **[docs/LLM_SETUP.md](./docs/LLM_SETUP.md)**

## ğŸ”§ Estructura del Proyecto

```
production/
â”œâ”€â”€ main.py                 # Script principal del pipeline
â”œâ”€â”€ README.md               # Esta documentaciÃ³n
â”œâ”€â”€ requirements.txt        # Dependencias Python necesarias
â”œâ”€â”€ .env.example            # Plantilla de configuraciÃ³n
â”œâ”€â”€ .env                    # Tu configuraciÃ³n (no en git)
â”‚
â”œâ”€â”€ config/                 # Configuraciones
â”‚   â””â”€â”€ config.py          # Config centralizada
â”‚
â”œâ”€â”€ core/                   # MÃ³dulos del pipeline
â”‚   â”œâ”€â”€ llm_provider.py    # Proveedor de LLM
â”‚   â”œâ”€â”€ fase_01_*.py       # Procesamiento bÃ¡sico
â”‚   â”œâ”€â”€ fase_02_*.py       # AnÃ¡lisis de sentimientos
â”‚   â”œâ”€â”€ fase_03_*.py       # AnÃ¡lisis de subjetividad
â”‚   â”œâ”€â”€ fase_04_*.py       # ClasificaciÃ³n categorÃ­as
â”‚   â”œâ”€â”€ fase_05_*.py       # AnÃ¡lisis de tÃ³picos (usa LLM)
â”‚   â””â”€â”€ fase_06_*.py       # ResÃºmenes (usa LLM)
â”‚
â”œâ”€â”€ scripts/                # Scripts de utilidad
â”‚   â”œâ”€â”€ install_dependencies.sh      # Script de instalaciÃ³n automÃ¡tica
â”‚   â”œâ”€â”€ setup_local_llm_completo.sh  # ğŸ†• Setup TODO-EN-UNO (recomendado)
â”‚   â”œâ”€â”€ setup_ollama.sh              # InstalaciÃ³n Ollama bÃ¡sica
â”‚   â”œâ”€â”€ test_llm_setup.py            # Test de configuraciÃ³n
â”‚   â””â”€â”€ compile_requirements.sh      # Compilar dependencias
â”‚
â”œâ”€â”€ docs/                   # DocumentaciÃ³n
â”‚   â”œâ”€â”€ INSTALL.md         # GuÃ­a de instalaciÃ³n detallada
â”‚   â”œâ”€â”€ LLM_SETUP.md       # GuÃ­a LLM completa
â”‚   â””â”€â”€ CHANGELOG.md       # Historial de cambios
â”‚
â”œâ”€â”€ data/                   # Datos de entrada/salida
â”‚   â”œâ”€â”€ dataset.csv
â”‚   â””â”€â”€ shared/
â”‚
â””â”€â”€ models/                 # Modelos BERT entrenados
    â”œâ”€â”€ multilabel_task/
    â””â”€â”€ subjectivity_task/
```

## ğŸ”§ Estructura del Pipeline

El sistema ejecuta 7 fases secuenciales:

1. **Procesamiento BÃ¡sico**: Limpieza y normalizaciÃ³n de datos
2. **AnÃ¡lisis de Sentimientos**: ClasificaciÃ³n Positivo/Negativo/Neutro
3. **AnÃ¡lisis de Subjetividad**: IdentificaciÃ³n de opiniones subjetivas
4. **ClasificaciÃ³n de CategorÃ­as**: Etiquetado multi-etiqueta con BERT
5. **AnÃ¡lisis de TÃ³picos**: IdentificaciÃ³n de sub-temas con BERTopic + LLM â­
6. **Resumen Inteligente**: GeneraciÃ³n de resÃºmenes con LLM â­
7. **Visualizaciones**: GeneraciÃ³n de grÃ¡ficos profesionales (dashboard, sentimientos, categorÃ­as, tÃ³picos, temporal)

â­ = Fases que utilizan LLM configurable

## ğŸ“ Archivos de ConfiguraciÃ³n

- **`.env`**: ConfiguraciÃ³n de LLM y variables de entorno
- **`.env.example`**: Plantilla de configuraciÃ³n
- **`config.py`**: ConfiguraciÃ³n centralizada del sistema
- **`llm_provider.py`**: AbstracciÃ³n de proveedores LLM

## ğŸ› ï¸ Comandos Ãštiles

### GestiÃ³n de Ollama

```bash
# Listar modelos instalados
ollama list

# Descargar un modelo
ollama pull llama3.2:3b

# Iniciar servidor
ollama serve

# Probar modelo
ollama run llama3.2:3b
```

### Verificar ConfiguraciÃ³n

```bash
# Ver configuraciÃ³n actual de LLM
python -c "from core.llm_provider import LLMProvider; print(LLMProvider.get_info())"

# Probar conexiÃ³n con LLM
python -c "from core.llm_provider import get_llm; llm = get_llm(); print(llm.invoke('Hola'))"
```

## ğŸ› SoluciÃ³n de Problemas

### Error: "Error al inicializar Ollama"

1. Verifica que Ollama estÃ© ejecutÃ¡ndose:
   ```bash
   ollama serve
   ```

2. Verifica que el modelo estÃ© descargado:
   ```bash
   ollama list
   ollama pull llama3.2:3b
   ```

### Error: "OPENAI_API_KEY no estÃ¡ configurado"

1. Crea/edita el archivo `.env` en `/production/`
2. Agrega tu API key:
   ```env
   LLM_MODE=api
   OPENAI_API_KEY=sk-proj-...
   ```

### Rendimiento Lento

1. **Para Ollama**: Usa un modelo mÃ¡s ligero
   ```bash
   ollama pull llama3.2:1b
   ```
   
2. **Para API**: Usa un modelo mÃ¡s rÃ¡pido
   ```env
   OPENAI_MODEL=gpt-3.5-turbo
   ```

## ğŸ“Š Salidas del Sistema

El pipeline genera los siguientes archivos:

- **`data/dataset.csv`**: Dataset procesado con todas las columnas aÃ±adidas
- **`data/shared/categorias_scores.json`**: Probabilidades de categorÃ­as
- **`data/shared/resumenes.json`**: ResÃºmenes generados por LLM

## ğŸ”„ Cambiar entre Modos

Para cambiar entre API y Local:

1. Edita `.env`:
   ```env
   # Para usar Ollama local
   LLM_MODE=local
   
   # Para usar OpenAI API
   LLM_MODE=api
   ```

2. Reinicia el pipeline

## ğŸ“ Ejemplo de Uso ProgramÃ¡tico

```python
from core.llm_provider import crear_chain, LLMProvider

# Ver configuraciÃ³n actual
print(LLMProvider.get_info())

# Crear una cadena simple
template = "Analiza esta opiniÃ³n turÃ­stica: {opinion}"
chain = crear_chain(template)

# Invocar
resultado = chain.invoke({
    "opinion": "El hotel es excelente, muy limpio y buena atenciÃ³n"
})
print(resultado)
```

## ğŸ¤ Contribuciones

Para mÃ¡s informaciÃ³n sobre el proyecto completo, consulta el README principal en el directorio raÃ­z.

## ğŸ“„ Licencia

Este proyecto es parte del AI Tourism Opinion Analyzer.
