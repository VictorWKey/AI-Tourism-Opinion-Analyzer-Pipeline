#!/usr/bin/env python
"""
Script de Prueba de Configuraci√≥n LLM
======================================
Verifica que la configuraci√≥n de LLM est√© correcta y funcionando.
"""

import sys
from pathlib import Path

# Agregar directorio production al path
sys.path.insert(0, str(Path(__file__).parent.parent))

def test_config():
    """Prueba la configuraci√≥n b√°sica."""
    print("=" * 60)
    print("PRUEBA DE CONFIGURACI√ìN LLM")
    print("=" * 60)
    print()
    
    try:
        from config import ConfigLLM
        
        print("‚úÖ M√≥dulo de configuraci√≥n cargado")
        print()
        
        # Mostrar configuraci√≥n
        info = ConfigLLM.get_info()
        print("üìã Configuraci√≥n actual:")
        print(f"   ‚Ä¢ Modo: {info['modo'].upper()}")
        print(f"   ‚Ä¢ Modelo: {info['modelo']}")
        
        if info['modo'] == 'api':
            print(f"   ‚Ä¢ API key configurada: {'‚úÖ' if info.get('api_key_configurada') else '‚ùå'}")
            if not info.get('api_key_configurada'):
                print()
                print("‚ö†Ô∏è  ADVERTENCIA: API key no configurada")
                print("   Edita el archivo .env y agrega tu OPENAI_API_KEY")
                return False
        else:
            print(f"   ‚Ä¢ URL base: {info['base_url']}")
        
        print(f"   ‚Ä¢ Temperatura: {info['temperatura']}")
        print()
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error al cargar configuraci√≥n: {e}")
        print()
        print("üí° Soluciones:")
        print("   1. Copia .env.example a .env")
        print("   2. Edita .env con tu configuraci√≥n")
        print("   3. Revisa LLM_SETUP.md para m√°s detalles")
        return False


def test_llm_provider():
    """Prueba el proveedor de LLM."""
    print("=" * 60)
    print("PRUEBA DE PROVEEDOR LLM")
    print("=" * 60)
    print()
    
    try:
        from core import LLMProvider
        
        print("‚è≥ Inicializando proveedor LLM...")
        provider = LLMProvider()
        print()
        
        info = provider.get_info()
        
        if info['modo'] == 'local':
            print("üîç Verificando conexi√≥n con Ollama...")
        else:
            print("üîç Verificando conexi√≥n con OpenAI API...")
        
        print()
        return True
        
    except Exception as e:
        print(f"‚ùå Error al inicializar proveedor: {e}")
        print()
        
        # Ayuda espec√≠fica seg√∫n el error
        error_str = str(e).lower()
        
        if 'ollama' in error_str or 'connect' in error_str:
            print("üí° Parece un problema con Ollama:")
            print("   1. Verifica que Ollama est√© instalado: ollama --version")
            print("   2. Inicia el servidor: ollama serve")
            print("   3. Descarga un modelo: ollama pull llama3.2:3b")
            print()
            print("   O ejecuta el script autom√°tico: ./setup_ollama.sh")
        
        elif 'openai' in error_str or 'api' in error_str:
            print("üí° Parece un problema con OpenAI API:")
            print("   1. Verifica tu API key en .env")
            print("   2. Verifica que langchain-openai est√© instalado")
            print("   3. Verifica tu cr√©dito en OpenAI: https://platform.openai.com/usage")
        
        else:
            print("üí° Error desconocido. Revisa:")
            print("   1. El archivo .env existe y est√° configurado")
            print("   2. Las dependencias est√°n instaladas: pip install -r requirements.txt")
            print("   3. Consulta LLM_SETUP.md para m√°s detalles")
        
        return False


def test_llm_inference():
    """Prueba la inferencia del LLM con un prompt simple."""
    print("=" * 60)
    print("PRUEBA DE INFERENCIA LLM")
    print("=" * 60)
    print()
    
    try:
        from core import crear_chain
        
        print("‚è≥ Creando cadena de prueba...")
        template = "Responde con exactamente una palabra: ¬øCu√°l es la capital de Francia?"
        chain = crear_chain(template)
        
        print("‚è≥ Ejecutando inferencia...")
        print()
        
        respuesta = chain.invoke({})
        
        print("üìù Respuesta del LLM:")
        print(f"   {respuesta}")
        print()
        
        # Verificar que la respuesta sea razonable
        if 'Par√≠s' in respuesta or 'Paris' in respuesta or 'paris' in respuesta.lower():
            print("‚úÖ Respuesta correcta detectada")
            return True
        else:
            print("‚ö†Ô∏è  Respuesta inesperada (pero el LLM funciona)")
            return True
        
    except Exception as e:
        print(f"‚ùå Error durante inferencia: {e}")
        print()
        print("üí° El LLM no pudo generar una respuesta.")
        print("   Revisa los errores anteriores para diagnosticar.")
        return False


def main():
    """Ejecuta todas las pruebas."""
    print()
    
    # Test 1: Configuraci√≥n
    success_config = test_config()
    print()
    
    if not success_config:
        print("‚ùå Pruebas detenidas debido a error de configuraci√≥n")
        print("   Corrige los errores anteriores y vuelve a ejecutar")
        sys.exit(1)
    
    # Test 2: Proveedor
    success_provider = test_llm_provider()
    print()
    
    if not success_provider:
        print("‚ùå Pruebas detenidas debido a error del proveedor")
        print("   Corrige los errores anteriores y vuelve a ejecutar")
        sys.exit(1)
    
    # Test 3: Inferencia
    success_inference = test_llm_inference()
    print()
    
    # Resumen final
    print("=" * 60)
    print("RESUMEN DE PRUEBAS")
    print("=" * 60)
    print()
    print(f"   Configuraci√≥n:  {'‚úÖ' if success_config else '‚ùå'}")
    print(f"   Proveedor:      {'‚úÖ' if success_provider else '‚ùå'}")
    print(f"   Inferencia:     {'‚úÖ' if success_inference else '‚ùå'}")
    print()
    
    if success_config and success_provider and success_inference:
        print("üéâ ¬°TODAS LAS PRUEBAS PASARON!")
        print()
        print("‚úÖ El sistema LLM est√° configurado correctamente")
        print("   Puedes ejecutar el pipeline: python main.py")
        print()
        sys.exit(0)
    else:
        print("‚ùå ALGUNAS PRUEBAS FALLARON")
        print()
        print("   Revisa los errores anteriores y consulta LLM_SETUP.md")
        print()
        sys.exit(1)


if __name__ == "__main__":
    main()
