# ============================================
# Configuración del Sistema LLM
# ============================================

# Modo de LLM: 'api' o 'local'
# - 'api': Usa OpenAI API (requiere OPENAI_API_KEY y costo por uso)
# - 'local': Usa Ollama localmente (gratuito, requiere instalación)
LLM_MODE=local

# ============================================
# Configuración OpenAI (Solo si LLM_MODE=api)
# ============================================
# Obtén tu API key en: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Modelo de OpenAI a usar
# Opciones: gpt-4o-mini, gpt-4o, gpt-3.5-turbo, etc.
OPENAI_MODEL=gpt-4o-mini

# ============================================
# Configuración Ollama (Solo si LLM_MODE=local)
# ============================================
# URL base de Ollama (por defecto localhost)
OLLAMA_BASE_URL=http://localhost:11434

# Modelo de Ollama a usar
# Modelos recomendados:
# - llama3.2:3b (rápido, ligero, 2GB RAM)
# - llama3.2:1b (muy rápido, muy ligero, 1GB RAM)
# - llama3.1:8b (mejor calidad, 4.7GB RAM)
# - gemma2:2b (alternativa ligera, 1.6GB RAM)
#
# Para descargar: ollama pull <modelo>
OLLAMA_MODEL=llama3.2:3b

# ============================================
# Parámetros de Generación (Compartidos)
# ============================================
# Temperatura (0 = determinístico, 1 = creativo)
LLM_TEMPERATURE=0
