# ðŸŽ‰ Sistema de Setup AutomÃ¡tico Completado

## âœ… Lo que se ha creado

### ðŸ“œ Script Principal: `setup_local_llm_completo.sh`

**UbicaciÃ³n**: `/production/scripts/setup_local_llm_completo.sh`

**Funcionalidad**:
- âœ… InstalaciÃ³n automÃ¡tica de Ollama (Linux/macOS)
- âœ… Inicio automÃ¡tico del servicio Ollama
- âœ… Descarga interactiva de modelos (3 opciones)
- âœ… ConfiguraciÃ³n automÃ¡tica del archivo `.env`
- âœ… VerificaciÃ³n completa con `test_llm_setup.py`
- âœ… Manejo de backups (preserva configuraciÃ³n anterior)
- âœ… Mensajes informativos con colores
- âœ… DetecciÃ³n de errores y ayuda de diagnÃ³stico

### ðŸ“š DocumentaciÃ³n Creada

1. **docs/SETUP_AUTOMATICO.md** - GuÃ­a completa del script
   - ExplicaciÃ³n detallada de cada paso
   - SoluciÃ³n de problemas comunes
   - Requisitos y recomendaciones
   - Ejemplos de uso

2. **README.md** - Actualizado
   - Nueva secciÃ³n "Setup AutomÃ¡tico TODO-EN-UNO"
   - Estructura de carpetas actualizada
   - Referencias al nuevo script

## ðŸš€ CÃ³mo Usar

### Uso BÃ¡sico (TODO AUTOMÃTICO)

```bash
cd production
./scripts/setup_local_llm_completo.sh
```

**Eso es todo**. El script:
1. Te pregunta quÃ© modelo quieres (3 opciones)
2. Hace toda la instalaciÃ³n y configuraciÃ³n
3. Prueba que todo funcione
4. Te dice si estÃ¡ listo o si hubo errores

### Modelos Disponibles

Durante la ejecuciÃ³n, elige:

- **OpciÃ³n 1**: `llama3.2:3b` (Recomendado)
  - 4GB RAM
  - RÃ¡pido
  - Buena calidad
  - **Mejor para desarrollo**

- **OpciÃ³n 2**: `llama3.1:8b` (Alta calidad)
  - 8GB RAM
  - MÃ¡s lento
  - Mejor calidad
  - **Mejor para producciÃ³n**

- **OpciÃ³n 3**: `gemma2:2b` (Ligero)
  - 2GB RAM
  - Muy rÃ¡pido
  - Calidad aceptable
  - **Mejor para equipos limitados**

## ðŸ”§ Lo que hace el Script (Detalles TÃ©cnicos)

### Paso 1: Verificar/Instalar Ollama
```bash
# Detecta si ollama estÃ¡ instalado
command -v ollama

# Si no estÃ¡, instala segÃºn el OS
# Linux: curl -fsSL https://ollama.com/install.sh | sh
# macOS: brew install ollama
```

### Paso 2: Iniciar Servicio
```bash
# Verifica si estÃ¡ corriendo
pgrep -x "ollama"

# Si no, lo inicia
nohup ollama serve > ollama.log 2>&1 &
```

### Paso 3: Descargar Modelo
```bash
# Pregunta al usuario quÃ© modelo
read -p "Selecciona el modelo [1-3]"

# Descarga el modelo elegido
ollama pull llama3.2:3b  # (o el que elijas)
```

### Paso 4: Configurar .env
```bash
# Hace backup del .env actual
cp .env .env.backup.YYYYMMDD_HHMMSS

# Crea nuevo .env con configuraciÃ³n local
cat > .env << EOF
USE_API=false
OLLAMA_MODEL=llama3.2:3b
OLLAMA_BASE_URL=http://localhost:11434
...
EOF
```

### Paso 5: Verificar
```bash
# Ejecuta el test de configuraciÃ³n
python scripts/test_llm_setup.py

# Verifica que todos los tests pasen
```

## ðŸ“ Archivos del Sistema

### Creados por el Script

```
production/
â”œâ”€â”€ .env                      # ConfiguraciÃ³n (creado/actualizado)
â”œâ”€â”€ .env.backup.20251108_123456  # Backup automÃ¡tico
â”œâ”€â”€ ollama.log                # Log del servicio Ollama
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ setup_local_llm_completo.sh  # ðŸ†• Script principal
    â”œâ”€â”€ setup_ollama.sh              # Script bÃ¡sico (ya existÃ­a)
    â””â”€â”€ test_llm_setup.py            # Test de verificaciÃ³n
```

### DocumentaciÃ³n

```
production/
â””â”€â”€ docs/
    â”œâ”€â”€ SETUP_AUTOMATICO.md   # ðŸ†• GuÃ­a del script automÃ¡tico
    â”œâ”€â”€ LLM_SETUP.md          # GuÃ­a completa LLM (ya existÃ­a)
    â””â”€â”€ CHANGELOG.md          # Historial de cambios
```

## âœ¨ Ventajas del Script AutomÃ¡tico

### Para el Usuario

1. **Un Solo Comando**: Todo se hace con una ejecuciÃ³n
2. **Sin ConfiguraciÃ³n Manual**: No hay que editar archivos
3. **VerificaciÃ³n AutomÃ¡tica**: Detecta errores inmediatamente
4. **Backups AutomÃ¡ticos**: Preserva configuraciÃ³n anterior
5. **Mensajes Claros**: Sabe exactamente quÃ© estÃ¡ pasando
6. **RecuperaciÃ³n de Errores**: Sugiere soluciones si algo falla

### Para Desarrolladores

1. **Reproducible**: Mismo resultado cada vez
2. **Idempotente**: Se puede ejecutar mÃºltiples veces
3. **DetecciÃ³n de Estado**: No reinstala si ya estÃ¡ listo
4. **Logging**: Guarda logs para debugging
5. **Portable**: Funciona en Linux y macOS
6. **Mantenible**: CÃ³digo bien documentado

## ðŸŽ¯ Casos de Uso

### 1. Primera InstalaciÃ³n
```bash
./scripts/setup_local_llm_completo.sh
# Hace TODO desde cero
```

### 2. Cambiar de Modelo
```bash
# El script detecta Ollama ya instalado
# Solo descarga el nuevo modelo y actualiza .env
./scripts/setup_local_llm_completo.sh
```

### 3. Reinstalar/Reparar
```bash
# Ejecutar nuevamente
# Verifica todo y arregla lo que estÃ© roto
./scripts/setup_local_llm_completo.sh
```

### 4. Verificar Estado
```bash
# El script tambiÃ©n sirve como diagnÃ³stico
# Muestra quÃ© estÃ¡ instalado y funcionando
./scripts/setup_local_llm_completo.sh
```

## ðŸ” VerificaciÃ³n Post-Setup

DespuÃ©s de ejecutar el script exitosamente:

```bash
# 1. Verificar que Ollama estÃ© corriendo
pgrep ollama
# Debe mostrar un nÃºmero (PID)

# 2. Ver modelos instalados
ollama list
# Debe mostrar el modelo que elegiste

# 3. Verificar configuraciÃ³n
cat .env
# Debe mostrar USE_API=false y el modelo correcto

# 4. Probar LLM
python -c "from core.llm_provider import LLMProvider; print(LLMProvider.get_info())"
# Debe mostrar config del modo local

# 5. Ejecutar pipeline
python main.py
# Debe funcionar sin errores
```

## ðŸ“Š ComparaciÃ³n: Antes vs Ahora

### Antes (Setup Manual)

```bash
# 1. Instalar Ollama manualmente
curl -fsSL https://ollama.com/install.sh | sh

# 2. Iniciar servicio
ollama serve &

# 3. Descargar modelo
ollama pull llama3.2:3b

# 4. Copiar .env.example
cp .env.example .env

# 5. Editar .env manualmente
nano .env
# USE_API=false
# OLLAMA_MODEL=llama3.2:3b
# ...

# 6. Probar manualmente
python scripts/test_llm_setup.py

# 7. Â¿FuncionÃ³? Si no, debug manual...
```

**Tiempo**: ~10-15 minutos  
**Pasos**: 7 pasos manuales  
**Probabilidad de error**: Alta (varios puntos de fallo)

### Ahora (Setup AutomÃ¡tico)

```bash
./scripts/setup_local_llm_completo.sh
# [Esperar 2-5 minutos]
# Â¡Listo!
```

**Tiempo**: ~2-5 minutos  
**Pasos**: 1 comando  
**Probabilidad de error**: Baja (manejo automÃ¡tico de errores)

## ðŸ›¡ï¸ Seguridad y Confiabilidad

### Backups AutomÃ¡ticos
- Antes de modificar `.env`, crea backup con timestamp
- Formato: `.env.backup.YYYYMMDD_HHMMSS`
- FÃ¡cil recuperaciÃ³n si algo sale mal

### DetecciÃ³n de Errores
- Verifica cada paso antes de continuar
- Mensajes claros si algo falla
- Sugerencias de soluciÃ³n automÃ¡ticas

### No Destructivo
- No borra archivos existentes
- No sobrescribe sin backup
- Preserva modelos ya descargados

## ðŸ“ PrÃ³ximos Pasos Recomendados

DespuÃ©s de ejecutar el script:

1. **Ejecutar el Pipeline**
   ```bash
   python main.py
   ```

2. **Ver Logs si hay Problemas**
   ```bash
   cat ollama.log
   ```

3. **Explorar Otros Modelos** (opcional)
   ```bash
   ollama pull llama3.1:8b
   # Luego edita .env: OLLAMA_MODEL=llama3.1:8b
   ```

4. **Leer la DocumentaciÃ³n Completa**
   ```bash
   cat docs/SETUP_AUTOMATICO.md
   cat docs/LLM_SETUP.md
   ```

## ðŸŽ“ Recursos Adicionales

- **DocumentaciÃ³n de Ollama**: https://ollama.com/docs
- **Modelos Disponibles**: https://ollama.com/library
- **LangChain con Ollama**: https://python.langchain.com/docs/integrations/llms/ollama

## ðŸ“ž Soporte

Si tienes problemas:

1. **Revisa el log**: `cat production/ollama.log`
2. **Consulta la guÃ­a**: `docs/SETUP_AUTOMATICO.md`
3. **Ejecuta el test**: `python scripts/test_llm_setup.py`
4. **Verifica Ollama**: `pgrep ollama` y `ollama list`

---

**Creado**: Noviembre 8, 2025  
**VersiÃ³n**: 1.0.0  
**Autor**: GitHub Copilot  
**Licencia**: Parte del proyecto AI-Tourism-Opinion-Analyzer
