# Configuraci√≥n de LLM para An√°lisis de Opiniones Tur√≠sticas

## üìå Opciones de LLM

El sistema soporta **dos modos** de funcionamiento:

### üåê Modo API (OpenAI)
- **Ventajas**: Mayor calidad de respuestas, sin requisitos de hardware
- **Desventajas**: Requiere API key de pago, costos por uso
- **Uso recomendado**: Producci√≥n con presupuesto disponible

### üíª Modo Local (Ollama)
- **Ventajas**: Completamente gratuito, privacidad total, sin l√≠mites de uso
- **Desventajas**: Requiere instalaci√≥n y recursos de hardware
- **Uso recomendado**: Desarrollo, pruebas, o producci√≥n sin presupuesto

---

## üöÄ Instalaci√≥n R√°pida

### 1. Instalar Dependencias Python

```bash
# Actualizar pip
pip install --upgrade pip

# Instalar dependencias
pip install -r requirements.txt

# O compilar desde requirements.in
pip-compile requirements.in
pip install -r requirements.txt
```

### 2. Configurar Variables de Entorno

```bash
# Copiar archivo de ejemplo
cp .env.example .env

# Editar .env con tu editor favorito
nano .env  # o vim, code, etc.
```

---

## ‚öôÔ∏è Configuraci√≥n por Modo

### üåê Configuraci√≥n para Modo API (OpenAI)

#### 1. Obtener API Key de OpenAI
1. Visita: https://platform.openai.com/api-keys
2. Crea una cuenta o inicia sesi√≥n
3. Genera una nueva API key
4. Copia la clave (formato: `sk-proj-...`)

#### 2. Configurar `.env`
```env
LLM_MODE=api
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
OPENAI_MODEL=gpt-4o-mini
```

#### Modelos Recomendados
| Modelo | Costo | Calidad | Velocidad | Uso Recomendado |
|--------|-------|---------|-----------|-----------------|
| `gpt-4o-mini` | üí∞ Bajo | ‚≠ê‚≠ê‚≠ê‚≠ê | üöÄ R√°pido | **Producci√≥n** (recomendado) |
| `gpt-3.5-turbo` | üí∞ Muy bajo | ‚≠ê‚≠ê‚≠ê | üöÄüöÄ Muy r√°pido | Desarrollo/Pruebas |
| `gpt-4o` | üí∞üí∞üí∞ Alto | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üêå Lento | An√°lisis cr√≠ticos |

#### Costos Aproximados (Mayo 2024)
- **gpt-4o-mini**: ~$0.15 USD por 1M tokens de entrada
- **gpt-3.5-turbo**: ~$0.50 USD por 1M tokens de entrada

---

### üíª Configuraci√≥n para Modo Local (Ollama)

#### 1. Instalar Ollama

##### Linux
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

##### macOS
```bash
brew install ollama
```

##### Windows
Descarga el instalador desde: https://ollama.ai/download

#### 2. Iniciar el Servidor de Ollama
```bash
ollama serve
```
> **Nota**: Deja esta terminal abierta mientras uses el sistema

#### 3. Descargar un Modelo

```bash
# Opci√≥n 1: Modelo ligero y r√°pido (RECOMENDADO para empezar)
ollama pull llama3.2:3b

# Opci√≥n 2: Modelo muy ligero (para equipos con poca RAM)
ollama pull llama3.2:1b

# Opci√≥n 3: Modelo de mayor calidad (requiere m√°s RAM)
ollama pull llama3.1:8b

# Opci√≥n 4: Alternativa ligera (Gemma)
ollama pull gemma2:2b
```

#### 4. Configurar `.env`
```env
LLM_MODE=local
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
```

#### Modelos Recomendados
| Modelo | RAM Req. | Velocidad | Calidad | Uso Recomendado |
|--------|----------|-----------|---------|-----------------|
| `llama3.2:1b` | 1 GB | üöÄüöÄüöÄ | ‚≠ê‚≠ê‚≠ê | Equipos limitados |
| `llama3.2:3b` | 2 GB | üöÄüöÄ | ‚≠ê‚≠ê‚≠ê‚≠ê | **Balanceado** (recomendado) |
| `llama3.1:8b` | 4.7 GB | üöÄ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Alta calidad |
| `gemma2:2b` | 1.6 GB | üöÄüöÄ | ‚≠ê‚≠ê‚≠ê‚≠ê | Alternativa ligera |

#### Requisitos de Hardware
- **M√≠nimo**: 4 GB RAM, CPU moderna
- **Recomendado**: 8 GB RAM, GPU NVIDIA (opcional, mejora velocidad)
- **√ìptimo**: 16 GB RAM, GPU NVIDIA con CUDA

---

## üß™ Verificar Instalaci√≥n

### Verificar Ollama
```bash
# Listar modelos instalados
ollama list

# Probar un modelo
ollama run llama3.2:3b
>>> Hola, ¬øc√≥mo est√°s?
>>> /bye
```

### Verificar Python
```bash
# Dentro del directorio production/
python -c "from llm_provider import LLMProvider; print(LLMProvider.get_info())"
```

Salida esperada:
```json
{
  "modo": "local",
  "temperatura": 0.0,
  "max_tokens": 2000,
  "modelo": "llama3.2:3b",
  "base_url": "http://localhost:11434"
}
```

---

## üîÑ Cambiar entre Modos

### En Tiempo de Ejecuci√≥n
No soportado actualmente. Debes reiniciar el programa.

### Cambiar Configuraci√≥n
Edita el archivo `.env` y modifica `LLM_MODE`:

```env
# Para usar API de OpenAI
LLM_MODE=api

# Para usar Ollama local
LLM_MODE=local
```

---

## üêõ Soluci√≥n de Problemas

### Problema: "Error al inicializar Ollama"
**Soluci√≥n**:
1. Verifica que Ollama est√© ejecut√°ndose:
   ```bash
   ollama serve
   ```
2. Verifica que el modelo est√© descargado:
   ```bash
   ollama list
   ollama pull llama3.2:3b
   ```

### Problema: "OPENAI_API_KEY no est√° configurado"
**Soluci√≥n**:
1. Verifica que el archivo `.env` existe en `/production/`
2. Verifica que contiene `OPENAI_API_KEY=sk-proj-...`
3. Reinicia el programa

### Problema: "No se puede conectar con Ollama"
**Soluci√≥n**:
1. Verifica que el puerto 11434 est√© libre:
   ```bash
   lsof -i :11434  # Linux/Mac
   netstat -ano | findstr :11434  # Windows
   ```
2. Verifica la URL en `.env`:
   ```env
   OLLAMA_BASE_URL=http://localhost:11434
   ```

### Problema: Respuestas de baja calidad con Ollama
**Soluci√≥n**:
1. Prueba un modelo m√°s grande:
   ```bash
   ollama pull llama3.1:8b
   ```
2. Actualiza `.env`:
   ```env
   OLLAMA_MODEL=llama3.1:8b
   ```

---

## üìä Comparaci√≥n de Rendimiento

| Aspecto | Modo API | Modo Local |
|---------|----------|------------|
| **Costo** | üí∞ Por uso | ‚úÖ Gratuito |
| **Calidad** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Velocidad** | üöÄüöÄ | üöÄ (depende del hardware) |
| **Privacidad** | ‚ö†Ô∏è Datos enviados a OpenAI | ‚úÖ 100% local |
| **Requisitos** | Internet, API key | Ollama instalado, RAM |
| **Escalabilidad** | ‚úÖ Ilimitada | ‚ö†Ô∏è Limitada por hardware |

---

## üí° Recomendaciones

### Para Desarrollo/Pruebas
- **Usa Ollama** con `llama3.2:3b`
- Gratis, r√°pido, suficiente calidad

### Para Producci√≥n con Presupuesto
- **Usa OpenAI API** con `gpt-4o-mini`
- Mejor calidad, sin requisitos de hardware

### Para Producci√≥n sin Presupuesto
- **Usa Ollama** con `llama3.1:8b`
- Requiere servidor dedicado con buena RAM

### Para Equipos Limitados
- **Usa Ollama** con `llama3.2:1b` o `gemma2:2b`
- Funciona en laptops con 4GB RAM

---

## üìù Ejemplo de Uso

```python
from core.llm_provider import LLMProvider, crear_chain

# El sistema carga autom√°ticamente la configuraci√≥n desde .env
provider = LLMProvider()

# Ver configuraci√≥n actual
print(provider.get_info())

# Crear una cadena simple
template = "Responde en espa√±ol: {pregunta}"
chain = crear_chain(template)

# Invocar
respuesta = chain.invoke({"pregunta": "¬øQu√© es el turismo sostenible?"})
print(respuesta)
```

---

## üÜò Soporte

Para m√°s informaci√≥n sobre:
- **Ollama**: https://ollama.ai/docs
- **OpenAI API**: https://platform.openai.com/docs
- **Langchain**: https://python.langchain.com/docs

---

## üìÑ Licencia

Este componente es parte del proyecto AI Tourism Opinion Analyzer.
