# üöÄ Gu√≠a R√°pida: Setup Autom√°tico de LLM Local

## ¬øQu√© hace este script?

`setup_local_llm_completo.sh` es un script **TODO-EN-UNO** que configura completamente tu LLM local en minutos.

## ‚ú® Caracter√≠sticas

- ‚úÖ **Instalaci√≥n Autom√°tica**: Detecta tu sistema e instala Ollama
- ‚úÖ **Descarga de Modelos**: Te permite elegir el modelo que mejor se adapte a tu hardware
- ‚úÖ **Configuraci√≥n Autom√°tica**: Crea y configura el archivo `.env` por ti
- ‚úÖ **Verificaci√≥n Completa**: Prueba que todo funcione antes de terminar
- ‚úÖ **Sin Intervenci√≥n**: Solo ejecutas una vez y el script hace todo

## üéØ Uso

### Ejecuci√≥n B√°sica

```bash
cd production
./scripts/setup_local_llm_completo.sh
```

### Opciones de Modelos

Durante la ejecuci√≥n, el script te preguntar√° qu√© modelo quieres:

1. **llama3.2:3b** - Recomendado (4GB RAM, r√°pido, buena calidad)
2. **llama3.1:8b** - Mejor calidad (8GB RAM, m√°s lento)
3. **gemma2:2b** - M√°s ligero (2GB RAM, muy r√°pido, menor calidad)

**Solo presiona el n√∫mero y Enter**. Si no eliges nada, usa llama3.2:3b autom√°ticamente.

## üìã Proceso Completo

El script ejecuta estos 5 pasos:

### Paso 1: Verificar/Instalar Ollama
- Detecta si ya tienes Ollama instalado
- Si no, lo instala autom√°ticamente (Linux/macOS)

### Paso 2: Iniciar Servicio Ollama
- Verifica si Ollama est√° corriendo
- Si no, lo inicia en segundo plano
- Crea archivo de log en `production/ollama.log`

### Paso 3: Descargar Modelo LLM
- Te muestra las opciones disponibles
- Descarga el modelo que elijas
- Verifica que se instal√≥ correctamente

### Paso 4: Configurar .env
- Hace backup de tu `.env` actual (si existe)
- Crea nuevo `.env` configurado para modo local
- Establece todas las variables necesarias

### Paso 5: Probar Configuraci√≥n
- Ejecuta `scripts/test_llm_setup.py`
- Verifica que todo funcione correctamente
- Te muestra un resumen final

## üìä Salida Esperada

```
============================================================
  CONFIGURACI√ìN COMPLETA DE LLM LOCAL (OLLAMA)
============================================================

[PASO 1/5] Verificando instalaci√≥n de Ollama...
‚úì Ollama ya est√° instalado

[PASO 2/5] Iniciando servicio Ollama...
‚úì Ollama ya est√° corriendo

[PASO 3/5] Descargando modelo de LLM...
Modelos disponibles:
  1. llama3.2:3b  - Recomendado para desarrollo (4GB RAM, r√°pido)
  2. llama3.1:8b  - Mejor calidad (8GB RAM, m√°s lento)
  3. gemma2:2b    - M√°s ligero (2GB RAM, muy r√°pido)

Selecciona el modelo [1-3]: 1
‚úì Modelo llama3.2:3b descargado correctamente

[PASO 4/5] Configurando archivo .env...
‚úì Archivo .env configurado para modo local

[PASO 5/5] Probando configuraci√≥n...
‚úÖ CONFIGURACI√ìN COMPLETA EXITOSA
```

## ‚ö†Ô∏è Requisitos Previos

### M√≠nimos
- **Sistema**: Linux o macOS
- **RAM**: 4GB m√≠nimo (8GB recomendado)
- **Disco**: 3-6GB libres para Ollama + modelo
- **Python**: 3.10+ instalado
- **Dependencias Python**: `pip install -r requirements.txt`

### Verificar antes de ejecutar

```bash
# Verificar Python
python --version  # Debe ser 3.10+

# Verificar pip
pip --version

# Verificar espacio en disco
df -h ~
```

## üêõ Soluci√≥n de Problemas

### Error: "Sistema operativo no soportado"

**Windows no est√° soportado** por el script autom√°tico. Opciones:

1. Usar WSL2 (Windows Subsystem for Linux)
2. Instalar Ollama manualmente desde https://ollama.com/download
3. Usar modo API en lugar de local

### Error: "Error al iniciar Ollama"

```bash
# Ver el log de errores
cat production/ollama.log

# Intentar iniciar manualmente
ollama serve
```

### Error: "Error al descargar el modelo"

```bash
# Verificar conexi√≥n a internet
ping -c 3 ollama.com

# Intentar descargar manualmente
ollama pull llama3.2:3b

# Listar modelos descargados
ollama list
```

### Error: "M√≥dulos Python no encontrados"

```bash
# Instalar dependencias
pip install -r requirements.txt

# Verificar instalaci√≥n
pip list | grep langchain
```

### El test final falla

```bash
# Verificar que Ollama est√© corriendo
pgrep ollama

# Probar conexi√≥n manual
curl http://localhost:11434/api/tags

# Ver configuraci√≥n actual
cat .env

# Ejecutar test manualmente
python scripts/test_llm_setup.py
```

## üîÑ Cambiar de Modelo Despu√©s

Si quieres cambiar el modelo m√°s tarde:

```bash
# 1. Descargar nuevo modelo
ollama pull llama3.1:8b

# 2. Editar .env
nano .env
# Cambiar: OLLAMA_MODEL=llama3.1:8b

# 3. Probar
python scripts/test_llm_setup.py
```

## üìù Archivos Creados/Modificados

El script crea o modifica estos archivos:

- **`.env`**: Configuraci√≥n principal (backup del anterior)
- **`.env.backup.YYYYMMDD_HHMMSS`**: Backup de tu .env anterior
- **`ollama.log`**: Log del servicio Ollama

## üéì Siguiente Paso

Despu√©s de ejecutar el script exitosamente:

```bash
# Ejecutar el pipeline completo
python main.py
```

## üìö Documentaci√≥n Relacionada

- [README.md](../README.md) - Documentaci√≥n principal del proyecto
- [docs/LLM_SETUP.md](../docs/LLM_SETUP.md) - Gu√≠a completa de configuraci√≥n LLM
- [docs/CHANGELOG.md](../docs/CHANGELOG.md) - Historial de cambios

## üí° Consejos

### Para Desarrollo
- Usa `llama3.2:3b` - Perfecto balance entre velocidad y calidad
- Consume ~4GB RAM
- Respuestas en 2-5 segundos

### Para Producci√≥n
- Usa `llama3.1:8b` si tienes recursos
- Mejor calidad de respuestas
- Requiere servidor con 8GB+ RAM

### Para Equipos Limitados
- Usa `gemma2:2b` 
- Solo 2GB RAM necesarios
- Calidad aceptable para pruebas

## üÜò Soporte

Si tienes problemas:

1. **Revisa la secci√≥n de Soluci√≥n de Problemas** arriba
2. **Consulta el log**: `cat production/ollama.log`
3. **Ejecuta el test**: `python scripts/test_llm_setup.py`
4. **Documentaci√≥n de Ollama**: https://ollama.com/docs

## üîê Privacidad

Al usar Ollama local:
- ‚úÖ Tus datos **NUNCA** salen de tu computadora
- ‚úÖ Sin env√≠o de informaci√≥n a terceros
- ‚úÖ Control total sobre tus datos
- ‚úÖ Ideal para datos sensibles o confidenciales

---

**√öltima actualizaci√≥n**: Noviembre 2025  
**Versi√≥n del script**: 1.0.0
