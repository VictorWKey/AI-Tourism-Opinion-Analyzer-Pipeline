# Changelog - Sistema LLM Flexible

## [2.0.0] - 2025-11-08

### üéØ Cambios Principales

#### Nuevo Sistema de LLM Abstracto
- **Soporte para m√∫ltiples proveedores de LLM**:
  - ‚úÖ OpenAI API (modo `api`)
  - ‚úÖ Ollama Local (modo `local`)
  
- **Configuraci√≥n centralizada** mediante variables de entorno:
  - Archivo `.env` para configuraci√≥n
  - Cambio de modo sin modificar c√≥digo
  
- **M√≥dulo `llm_provider.py`** para abstracci√≥n de LLM:
  - Interfaz unificada para ambos modos
  - Patr√≥n Singleton para reutilizaci√≥n de conexiones
  - Funciones de conveniencia para crear chains

#### Archivos Nuevos

##### Configuraci√≥n
- `config.py`: Configuraci√≥n centralizada del sistema
- `.env.example`: Plantilla de configuraci√≥n con documentaci√≥n
- `.gitignore`: Exclusi√≥n de archivos sensibles

##### Documentaci√≥n
- `LLM_SETUP.md`: Gu√≠a completa de instalaci√≥n y configuraci√≥n
- `README.md`: Documentaci√≥n del pipeline de producci√≥n
- `CHANGELOG.md`: Este archivo

##### Scripts
- `setup_ollama.sh`: Instalaci√≥n autom√°tica de Ollama (Linux/macOS)
- `test_llm_setup.py`: Script de prueba de configuraci√≥n

#### Archivos Modificados

##### Fase 05 - An√°lisis Jer√°rquico de T√≥picos (`fase_05_analisis_jerarquico_topicos.py`)
- ‚úÖ Migrado de `langchain_openai.ChatOpenAI` a `llm_provider.crear_chain()`
- ‚úÖ Eliminadas dependencias directas de OpenAI
- ‚úÖ Soporta ambos modos de LLM (API/Local)

##### Fase 06 - Resumen Inteligente (`fase_06_resumen_inteligente.py`)
- ‚úÖ Migrado de `langchain_openai.ChatOpenAI` a `llm_provider.get_llm()`
- ‚úÖ Refactorizaci√≥n de m√©todos de generaci√≥n de res√∫menes
- ‚úÖ Soporta ambos modos de LLM (API/Local)

##### Pipeline Principal (`main.py`)
- ‚úÖ Agregada visualizaci√≥n de configuraci√≥n LLM al inicio
- ‚úÖ Validaci√≥n de configuraci√≥n antes de ejecutar
- ‚úÖ Mensajes de error informativos

##### Dependencias (`requirements.in`)
- ‚úÖ Agregado `langchain-ollama` para soporte local
- ‚úÖ Agregados `nltk` y `spacy` (antes faltaban)
- ‚úÖ Corregido formato de `fastopic` a URL de GitHub

---

## Compatibilidad con Versiones Anteriores

### ‚ö†Ô∏è BREAKING CHANGES

#### Variables de Entorno Requeridas
**Antes**: Solo se necesitaba `OPENAI_API_KEY`

**Ahora**: Se requiere archivo `.env` con al menos:
```env
LLM_MODE=api  # o 'local'
```

#### Dependencias Adicionales
**Antes**: Solo `langchain-openai`

**Ahora**: 
- `langchain-openai` (para modo API)
- `langchain-ollama` (para modo local)

### üîÑ Migraci√≥n desde v1.x

1. **Crear archivo `.env`**:
   ```bash
   cp .env.example .env
   ```

2. **Configurar modo API** (comportamiento anterior):
   ```env
   LLM_MODE=api
   OPENAI_API_KEY=tu-api-key
   OPENAI_MODEL=gpt-4o-mini
   ```

3. **Instalar nuevas dependencias**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Ejecutar prueba**:
   ```bash
   python test_llm_setup.py
   ```

---

## Beneficios del Nuevo Sistema

### üÜì Modo Local (Ollama)
- ‚úÖ **Costo cero**: Sin cargos por uso
- ‚úÖ **Privacidad total**: Datos 100% locales
- ‚úÖ **Sin l√≠mites**: Procesamiento ilimitado
- ‚ö†Ô∏è Requiere hardware (2-8 GB RAM seg√∫n modelo)

### üåê Modo API (OpenAI)
- ‚úÖ **Sin requisitos de hardware**: Funciona en cualquier equipo
- ‚úÖ **Alta calidad**: Mejores modelos disponibles
- ‚úÖ **Escalabilidad**: Procesamiento en la nube
- ‚ö†Ô∏è Costo por uso (~$0.15 por 1M tokens con gpt-4o-mini)

### üîß Flexibilidad
- ‚úÖ Cambio de modo sin modificar c√≥digo
- ‚úÖ Configuraci√≥n por variables de entorno
- ‚úÖ F√°cil integraci√≥n de nuevos proveedores
- ‚úÖ Abstracci√≥n unificada para ambos modos

---

## Ejemplos de Uso

### Ejemplo 1: Usar con Ollama Local
```bash
# 1. Configurar
echo "LLM_MODE=local" > .env
echo "OLLAMA_MODEL=llama3.2:3b" >> .env

# 2. Instalar Ollama
./setup_ollama.sh

# 3. Ejecutar
python main.py
```

### Ejemplo 2: Usar con OpenAI API
```bash
# 1. Configurar
echo "LLM_MODE=api" > .env
echo "OPENAI_API_KEY=sk-proj-..." >> .env

# 2. Ejecutar
python main.py
```

### Ejemplo 3: Uso Program√°tico
```python
from llm_provider import crear_chain, LLMProvider

# Ver configuraci√≥n
print(LLMProvider.get_info())

# Crear chain
chain = crear_chain("Analiza: {texto}")

# Invocar
resultado = chain.invoke({"texto": "Excelente hotel"})
```

---

## Pr√≥ximas Mejoras (Roadmap)

### v2.1.0 (Planificado)
- [ ] Soporte para m√°s proveedores (Anthropic Claude, Google Gemini)
- [ ] Cache de respuestas LLM para optimizaci√≥n
- [ ] Modo h√≠brido (combinar local + API)
- [ ] Panel de configuraci√≥n interactivo

### v2.2.0 (Planificado)
- [ ] M√©tricas de costo y uso por sesi√≥n
- [ ] Comparaci√≥n autom√°tica de calidad entre modelos
- [ ] Fine-tuning de modelos locales con datos propios
- [ ] Soporte para modelos cuantizados (GGUF)

---

## Recursos

### Documentaci√≥n
- [LLM_SETUP.md](./LLM_SETUP.md): Gu√≠a completa de configuraci√≥n
- [README.md](./README.md): Documentaci√≥n del pipeline

### Enlaces Externos
- [Ollama](https://ollama.ai): Instalaci√≥n y modelos
- [OpenAI Platform](https://platform.openai.com): API keys y documentaci√≥n
- [Langchain](https://python.langchain.com): Framework LLM

---

## Contribuciones

Este sistema fue dise√±ado para ser extensible. Para agregar un nuevo proveedor:

1. Implementar m√©todo en `LLMProvider` (ej: `_inicializar_anthropic()`)
2. Agregar configuraci√≥n en `ConfigLLM`
3. Actualizar documentaci√≥n
4. Crear tests

---

## Cr√©ditos

**Desarrollado por**: VictorWKey  
**Fecha**: 8 de Noviembre, 2025  
**Proyecto**: AI Tourism Opinion Analyzer  
